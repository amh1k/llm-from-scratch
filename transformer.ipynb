{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ed3f017",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "inputs = torch.tensor(\n",
    "[[0.43, 0.15, 0.89], # Your (x^1)\n",
    "[0.55, 0.87, 0.66], # journey (x^2)\n",
    "[0.57, 0.85, 0.64], # starts (x^3)\n",
    "[0.22, 0.58, 0.33], # with (x^4)\n",
    "[0.77, 0.25, 0.10], # one (x^5)\n",
    "[0.05, 0.80, 0.55]] # step (x^6)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "786d7404",
   "metadata": {},
   "source": [
    "what we do is in the above cell we have some words each of embedding size 3\n",
    "Then we send a query and compute the similarity of this query to every other word in torch.tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c95593ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865])\n",
      "Attention weights: tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
      "Sum: tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "query = inputs[1]\n",
    "attn_scores_2 = torch.empty(inputs.shape[0])\n",
    "for i, x in enumerate(inputs):\n",
    "    attn_scores_2[i] = torch.dot(x, query)\n",
    "    \n",
    "print(attn_scores_2)\n",
    "attn_weights_2 = torch.softmax(attn_scores_2, dim=0)\n",
    "print(\"Attention weights:\", attn_weights_2)\n",
    "print(\"Sum:\", attn_weights_2.sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "983c7785",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4419, 0.6515, 0.5683])\n"
     ]
    }
   ],
   "source": [
    "query = inputs[1] #the second word i.e token is our query\n",
    "context_vec_2 = torch.zeros(query.shape)\n",
    "for i, x in enumerate(inputs):\n",
    "    context_vec_2 += attn_weights_2[i] * x\n",
    "print(context_vec_2)\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c8928ef",
   "metadata": {},
   "source": [
    "Now instead of computing attention score for one query we compute attention score with all queries\n",
    "i.e we compute for each token its dot product with all other tokens to compute attention scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a0c156c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
      "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
      "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
      "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
      "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
      "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n"
     ]
    }
   ],
   "source": [
    "attn_scores = torch.empty(6,6)\n",
    "for i, x_i in enumerate(inputs):\n",
    "    for j, x_j in enumerate(inputs):\n",
    "        attn_scores[i,j] = torch.dot(x_i, x_j)\n",
    "print(attn_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c922efd5",
   "metadata": {},
   "source": [
    "Same task can be done with matrix multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7431f0ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
      "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
      "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
      "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
      "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
      "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n"
     ]
    }
   ],
   "source": [
    "attn_scores = inputs @inputs.T\n",
    "print(attn_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6e9ec10c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2098, 0.2006, 0.1981, 0.1242, 0.1220, 0.1452],\n",
      "        [0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581],\n",
      "        [0.1390, 0.2369, 0.2326, 0.1242, 0.1108, 0.1565],\n",
      "        [0.1435, 0.2074, 0.2046, 0.1462, 0.1263, 0.1720],\n",
      "        [0.1526, 0.1958, 0.1975, 0.1367, 0.1879, 0.1295],\n",
      "        [0.1385, 0.2184, 0.2128, 0.1420, 0.0988, 0.1896]])\n"
     ]
    }
   ],
   "source": [
    "attn_weights= torch.softmax(attn_scores, dim=-1)\n",
    "print(attn_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce9ee648",
   "metadata": {},
   "source": [
    "We will implement the self-attention mechanism step by step by introducing the\n",
    "three trainable weight matrices Wq , Wk , and Wv . These three matrices are used to\n",
    "project the embedded input tokens, x(i) , into query, key, and value vectors, respec-\n",
    "tively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "578631eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4421, 0.5931, 0.5790],\n",
      "        [0.4419, 0.6515, 0.5683],\n",
      "        [0.4431, 0.6496, 0.5671],\n",
      "        [0.4304, 0.6298, 0.5510],\n",
      "        [0.4671, 0.5910, 0.5266],\n",
      "        [0.4177, 0.6503, 0.5645]])\n"
     ]
    }
   ],
   "source": [
    "all_context_vecs= attn_weights @ inputs\n",
    "print(all_context_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "334ac786",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_2 = inputs[1] #the input token which in this case is the second word\n",
    "d_in = inputs.shape[1]\n",
    "d_out = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b9f073c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 3])\n",
      "torch.Size([3, 2])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "W_query = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "W_key = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "W_value = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "query_2 = x_2@W_query\n",
    "key_2 = x_2@W_key\n",
    "value_2=x_2@W_value\n",
    "print(inputs.shape)\n",
    "print(W_value.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "760bc4a5",
   "metadata": {},
   "source": [
    "Even though in this case our goal is only one query and to find the attention of all other words wrt to second word we still have to find keys and value vectors of all other words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "72d1d967",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keys.shape:  torch.Size([6, 2])\n",
      "values.shape:  torch.Size([6, 2])\n"
     ]
    }
   ],
   "source": [
    "keys = inputs @W_key\n",
    "values = inputs @W_value\n",
    "print('keys.shape: ', keys.shape)\n",
    "print('values.shape: ', values.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a498de",
   "metadata": {},
   "source": [
    "first lets compute similarity between second words query and second words key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "99b8ff60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.2705, 1.8524, 1.8111, 1.0795, 0.5577, 1.5440])\n"
     ]
    }
   ],
   "source": [
    "keys_2 = keys[1]\n",
    "attn_score_22= query_2.dot(keys_2)\n",
    "#We can genealize this\n",
    "attn_scores_2 = query_2 @keys.T\n",
    "print(attn_scores_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "32a4b653",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1500, 0.2264, 0.2199, 0.1311, 0.0906, 0.1820])\n",
      "tensor([0.3061, 0.8210])\n"
     ]
    }
   ],
   "source": [
    "d_k = keys.shape[-1]\n",
    "attn_weights_2 = torch.softmax(attn_scores_2/ d_k**0.5, dim = -1)\n",
    "print(attn_weights_2)\n",
    "context_vec_2 = attn_weights_2 @ values\n",
    "print(context_vec_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "079ef450",
   "metadata": {},
   "source": [
    "Now we consolidate all this functionality into one class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a6b4d2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class SelfAttentionV1(nn.Module):\n",
    "    def __init__(self, d_in, d_out):\n",
    "        super().__init__()\n",
    "        self.W_query= nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.W_value = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.W_key= nn.Parameter(torch.rand(d_in, d_out))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        keys = x @self.W_key\n",
    "        queries = x@self.W_query\n",
    "        values= x@self.W_value\n",
    "        attn_scores= queries @keys.T\n",
    "        attn_weights = torch.softmax(attn_scores/ keys.shape[-1]**0.5, dim=-1)\n",
    "        context_vec = attn_weights @ values\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fc44a23b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3507, 0.8808],\n",
      "        [0.3566, 0.8973],\n",
      "        [0.3563, 0.8966],\n",
      "        [0.3464, 0.8692],\n",
      "        [0.3446, 0.8644],\n",
      "        [0.3502, 0.8795]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "sa_v1 = SelfAttentionV1(d_in, d_out)\n",
    "print(sa_v1(inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22030e27",
   "metadata": {},
   "source": [
    "Instead of using nn.Parameter we can use nn.Linear "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "40a37ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class SelfAttentionV2(nn.Module):\n",
    "    def __init__(self, d_in, d_out, hasBias = False):\n",
    "        super().__init__()\n",
    "        self.W_query= nn.Linear(d_in, d_out, bias=hasBias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=hasBias)\n",
    "        self.W_key= nn.Linear(d_in, d_out, bias=hasBias)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        values= self.W_value(x)\n",
    "        attn_scores= queries @keys.T\n",
    "        attn_weights = torch.softmax(attn_scores/ keys.shape[-1]**0.5, dim=-1)\n",
    "        context_vec = attn_weights @ values\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f5f8df79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0293, -0.3242],\n",
      "        [ 0.0284, -0.3250],\n",
      "        [ 0.0284, -0.3251],\n",
      "        [ 0.0271, -0.3272],\n",
      "        [ 0.0267, -0.3278],\n",
      "        [ 0.0277, -0.3262]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(789)\n",
    "sa_v2 = SelfAttentionV2(d_in, d_out)\n",
    "print(sa_v2(inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7afecf76",
   "metadata": {},
   "source": [
    "Now we will use masking to hide all the next tokens so that a token has only previous tokens visible and it cant see next token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e031b6a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1667, 0.1635, 0.1641, 0.1672, 0.1767, 0.1618],\n",
      "        [0.1675, 0.1622, 0.1626, 0.1688, 0.1746, 0.1642],\n",
      "        [0.1675, 0.1623, 0.1627, 0.1687, 0.1745, 0.1643],\n",
      "        [0.1672, 0.1642, 0.1644, 0.1680, 0.1705, 0.1657],\n",
      "        [0.1672, 0.1646, 0.1648, 0.1678, 0.1694, 0.1661],\n",
      "        [0.1673, 0.1634, 0.1637, 0.1682, 0.1723, 0.1650]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "queries = sa_v2.W_query(inputs)\n",
    "keys = sa_v2.W_key(inputs)\n",
    "attn_scores = queries @ keys.T\n",
    "attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "print(attn_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b667ad47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0.],\n",
      "        [1., 1., 1., 1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "context_length = attn_scores.shape[0]\n",
    "mask_simple = torch.tril(torch.ones(context_length, context_length))\n",
    "print(mask_simple)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "739eca71",
   "metadata": {},
   "source": [
    "Now, we can multiply this mask with the attention weights to zero-out the values above\n",
    "the diagonal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6ae75993",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1667, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1675, 0.1622, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1675, 0.1623, 0.1627, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1672, 0.1642, 0.1644, 0.1680, 0.0000, 0.0000],\n",
      "        [0.1672, 0.1646, 0.1648, 0.1678, 0.1694, 0.0000],\n",
      "        [0.1673, 0.1634, 0.1637, 0.1682, 0.1723, 0.1650]],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "masked_simple = attn_weights*mask_simple\n",
    "print(masked_simple)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e17a215",
   "metadata": {},
   "source": [
    "The third step is to renormalize the attention weights to sum up to 1 again in each\n",
    "row. We can achieve this by dividing each element in each row by the sum in each row:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "26b5b354",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5080, 0.4920, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3401, 0.3295, 0.3303, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2519, 0.2474, 0.2477, 0.2530, 0.0000, 0.0000],\n",
      "        [0.2005, 0.1975, 0.1976, 0.2012, 0.2032, 0.0000],\n",
      "        [0.1673, 0.1634, 0.1637, 0.1682, 0.1723, 0.1650]],\n",
      "       grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "masked_simple_normalized = masked_simple / masked_simple.sum(dim=-1, keepdim=True)\n",
    "print(masked_simple_normalized)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "383681ae",
   "metadata": {},
   "source": [
    "When we apply a mask and then renormalize the attention weights, it might initially\n",
    "appear that information from future tokens (which we intend to mask) could still influ-\n",
    "ence the current token because their values are part of the softmax calculation. How-\n",
    "ever, the key insight is that when we renormalize the attention weights after masking,what we’re essentially doing is recalculating the softmax over a smaller subset (since\n",
    "masked positions don’t contribute to the softmax value).\n",
    "The mathematical elegance of softmax is that despite initially including all positions\n",
    "in the denominator, after masking and renormalizing, the effect of the masked posi-\n",
    "tions is nullified—they don’t contribute to the softmax score in any meaningful way.\n",
    "In simpler terms, after masking and renormalization, the distribution of attention\n",
    "weights is as if it was calculated only among the unmasked positions to begin with.\n",
    "This ensures there’s no information leakage from future (or otherwise masked)\n",
    "tokens as we intended."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "596db556",
   "metadata": {},
   "source": [
    "Another way which is more efficient is that before taking the softmax of the attention weights we can make the upper diagonals as - inifinity\n",
    "When we take the softmax it will automatically make the upper diagonal as 0 along with normalizing as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "551607fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0634,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
      "        [-0.0921, -0.1375,    -inf,    -inf,    -inf,    -inf],\n",
      "        [-0.0908, -0.1357, -0.1322,    -inf,    -inf,    -inf],\n",
      "        [-0.0514, -0.0774, -0.0757, -0.0452,    -inf,    -inf],\n",
      "        [-0.0420, -0.0635, -0.0623, -0.0367, -0.0229,    -inf],\n",
      "        [-0.0672, -0.1005, -0.0980, -0.0596, -0.0254, -0.0873]],\n",
      "       grad_fn=<MaskedFillBackward0>)\n"
     ]
    }
   ],
   "source": [
    "mask = torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
    "masked = attn_scores.masked_fill(mask.bool(), -torch.inf)\n",
    "print(masked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2f296b68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5080, 0.4920, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3401, 0.3295, 0.3303, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2519, 0.2474, 0.2477, 0.2530, 0.0000, 0.0000],\n",
      "        [0.2005, 0.1975, 0.1976, 0.2012, 0.2032, 0.0000],\n",
      "        [0.1673, 0.1634, 0.1637, 0.1682, 0.1723, 0.1650]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "attn_weights = torch.softmax(masked / keys.shape[-1]**0.5, dim=1)\n",
    "print(attn_weights)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
